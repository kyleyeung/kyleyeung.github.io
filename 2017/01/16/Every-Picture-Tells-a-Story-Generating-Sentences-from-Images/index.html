<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Every Picture Tells a Story:Generating Sentences From Images | Curbing My Enthusiasm</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-47712474-1','auto');ga('send','pageview');
</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Every Picture Tells a Story:Generating Sentences From Images</h1><a id="logo" href="/.">Curbing My Enthusiasm</a><p class="description">You are free to do whatever you won't do.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Every Picture Tells a Story:Generating Sentences From Images</h1><div class="post-meta">Jan 16, 2017<span> | </span><span class="category"><a href="/categories/Deep-Learning/">Deep Learning</a></span></div><div class="post-content"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="main-contribution"><a class="header-anchor" href="#main-contribution">¶</a>Main Contribution</h2>
<ol>
<li>A dataset to study the problem of generating short descriptive sentences from images</li>
<li>A novel representation intermediate between images and sentences</li>
<li>A novel, discriminative approach that produces very good results at sentence annotation</li>
<li>Methods to use distributional semantics to cope with out of vocabulary words for illustration</li>
<li>A quantitative evaluation of sentence generation at a useful scale</li>
</ol>
<a id="more"></a>
<h2 id="approach"><a class="header-anchor" href="#approach">¶</a>Approach</h2>
<ul>
<li>Assume a space of Meanings that comes between the space of Sentences and the space of Images</li>
<li>Map each to the meaning space</li>
<li>Compare the results</li>
</ul>
<h3 id="mapping-image-to-meaning"><a class="header-anchor" href="#mapping-image-to-meaning">¶</a>Mapping Image to Meaning</h3>
<p>Representation of meaning: <strong>a triplet of &lt;object, action, scene&gt;</strong><br>
<strong>Why?</strong></p>
<blockquote>
<p>This triplet provides a holistic idea about what the image (resp. sentence) is about and what is most important.For the image, this is the part that people would talk about first; for the sentence, this is the structure that should be preserved in the tightest summary.</p>
</blockquote>
<p><strong>How to construct the triplet?</strong><br>
Each slot in the meaning representation can take a value from a set of discrete values.The edges correspond to the binary relationships between nodes.<br>
<img src="%5Cimages%5C1484534108821.png" alt="Alt text|middle"><br>
Objects: 23 nouns<br>
Actions: 15 values<br>
Scenes: 29 values</p>
<p><strong>How to do inference?</strong></p>
<blockquote>
<p>Having provided the potentials of the MRF, we use a greedy method to do inference. Inference involves finding the best selection of the discrete sets of values given the unary and binary potentials.</p>
</blockquote>
<h3 id="image-potentials"><a class="header-anchor" href="#image-potentials">¶</a>Image Potentials</h3>
<h4 id="node-potentials"><a class="header-anchor" href="#node-potentials">¶</a>Node Potentials</h4>
<p><strong>Image features</strong></p>
<ul>
<li>Felzenszwalb et al. detector responses</li>
<li>Hoiem et al. classification responses</li>
<li>Gist-based scene classification responses</li>
</ul>
<p><strong>Node features</strong><br>
Node features are built by fitting a linear SVM to predict each of the nodes independently on the image features.</p>
<blockquote>
<p>This is a number-of-nodes-dimensional vector and each element in this vector provides a score for a node given the image.</p>
</blockquote>
<p>(TLDR)Similar images are expected to have similar meanings, so a set of note potentials are computed as a combination of node features :</p>
<blockquote>
<ul>
<li>by matching image features, we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the image side</em>. By doing so, we have a representation of what the node features are for similar images.</li>
<li>by matching image features, we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the sentence side.</em> By doing so, we have a representation of what the sentence representation are for images that look like our image.</li>
<li>by matching those node features derived from classifiers and detectors (above), we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the image side</em>. By doing so, we have a representation of what the node features are for images that produce similar classifier and detector outputs.</li>
<li>by matching those node features derived from classifiers and detectors (above), we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the sentence side</em>. By doing so, we have a representation of what the sentence representation does for images that produce similar classifier and detector outputs.</li>
</ul>
</blockquote>
<h4 id="edge-potentials"><a class="header-anchor" href="#edge-potentials">¶</a>Edge Potentials</h4>
<p><strong>Two problems</strong></p>
<ol>
<li>Introducing a parameter for each edge results in unmanageable number of parameters</li>
<li>Estimates of the parameters for the majority of edges would be noisy</li>
</ol>
<p><strong>Solution</strong></p>
<ol>
<li>Control the number of parameters</li>
<li>Do smoothing</li>
</ol>
<p><strong>How to limit the number of parameters?</strong><br>
Use four different estimates for edges:</p>
<ul>
<li>The normalized frequency of the word A in our corpus, $f(A)$</li>
<li>The normalized frequency of the word B in our corpus, $f(B)$</li>
<li>The normalized frequency of (A and B) at the same time, $f(A, B)$</li>
<li>$f(A, B)\over f(A)f(B)$</li>
</ul>
<h4 id="sequence-potentials"><a class="header-anchor" href="#sequence-potentials">¶</a>Sequence Potentials</h4>
<p><strong>How to represent a sentence?</strong><br>
Curran &amp; Clark parser --&gt; dependency parse --&gt; (object, action) pairs</p>
<p><strong>How to measure similarity?</strong><br>
Lin Similarity Measure</p>
<p><strong>How to discover co-occurring actions?</strong><br>
Action Co-occurrence Score</p>
<p><strong>Node potentials</strong></p>
<blockquote>
<ol>
<li>First we compute the similarity of each object, scene, and action extracted from each sentence. This gives us the the first estimates for the potentials over the nodes. We call this the sentence node feature.</li>
<li>For each sentence, we also compute the average of sentence node features for other four sentences describing the same images in the train set.</li>
<li>We compute the average of k nearest neighbors in the sentence node features space for a given sentence. We consider this as our third estimate for nodes.</li>
<li>We also compute the average of the image node features for images corresponding to the nearest neighbors in the item above.</li>
<li>The average of the sentence node features of reference sentences for the nearest neighbors in the item 3 is considered as our fifth estimate for nodes.</li>
<li>We also include the sentence node feature for the reference sentence.</li>
</ol>
</blockquote>
<p><strong>Edge Potentials</strong><br>
Identical to those of images.</p>
<h4 id="learning"><a class="header-anchor" href="#learning">¶</a>Learning</h4>
<p>2 mappings to be learned:</p>
<ol>
<li>image space --&gt; meaning space, using image potentials</li>
<li>sentence space --&gt; meaning space, using sentence potentials</li>
</ol>
<p><strong>Structure learning problem</strong><br>
$$\min_{\omega} {\lambda\over2}\lVert\omega\rVert^2+{1\over n}\sum _{i\in examples}\xi <em>i$$<br>
subject to<br>
$$\omega\Phi(x_i,y_i)+\xi_i\ge\max</em>{y\in meaning\ space }\omega\Phi(x_i,y)+L(y_i,y)\ \forall i\in examples$$<br>
$$\xi_i\ge 0 \ \forall i \in examples$$</p>
<h2 id="evaluation"><a class="header-anchor" href="#evaluation">¶</a>Evaluation</h2>
<h3 id="dataset"><a class="header-anchor" href="#dataset">¶</a>Dataset</h3>
<p>Built from PASCAL 2008 images with Amazon Mechanical Turk</p>
<h3 id="inference"><a class="header-anchor" href="#inference">¶</a>Inference</h3>
</div><div class="tags"><a href="/tags/Image-Caption/">Image Caption</a><a href="/tags/Paper-Notes/">Paper Notes</a></div><div class="post-nav"><a class="pre" href="/2017/02/27/Paper-Excerpts-in-Image-Caption/">Paper Excerpts in Image Caption</a><a class="next" href="/2017/01/12/Show-and-Tell-Lessons-learned-from-the-2015-MSCOCO-Image-Captioning-Challenge/">Show and Tell: Lessons Learned From the 2015 MSCOCO Image Captioning Challenge</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Computational-Social-Science/">Computational Social Science</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hacking/">Hacking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life-Record/">Life Record</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Image-Caption/" style="font-size: 15px;">Image Caption</a> <a href="/tags/Paper-Notes/" style="font-size: 15px;">Paper Notes</a> <a href="/tags/Object-Detection/" style="font-size: 15px;">Object Detection</a> <a href="/tags/CTF/" style="font-size: 15px;">CTF</a> <a href="/tags/Birthday/" style="font-size: 15px;">Birthday</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/08/23/where-would-you-go/">Where'd You Go?</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/21/what-is-computational-social-science/">什么是计算社会科学？</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/18/生日快乐，凯尔先生2018/">生日快乐,凯尔先生</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/23/Curbing-my-enthusiasm/">Curbing My Enthusiasm</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/03/01/Faster-R-CNN/">Faster R-CNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/28/DenseCap-Fully-Convolutional-Localization-Networks-for-Dense-Captioning/">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/27/Paper-Excerpts-in-Image-Caption/">Paper Excerpts in Image Caption</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/16/Every-Picture-Tells-a-Story-Generating-Sentences-from-Images/">Every Picture Tells a Story:Generating Sentences From Images</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/12/Show-and-Tell-Lessons-learned-from-the-2015-MSCOCO-Image-Captioning-Challenge/">Show and Tell: Lessons Learned From the 2015 MSCOCO Image Captioning Challenge</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/05/My-CTF-Journey-i春秋《戏说春秋》/">My CTF Journey:i春秋《戏说春秋》</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Curbing My Enthusiasm.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>