<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Every Picture Tells a Story:Generating Sentences from Images | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Main Contribution A dataset to study the problem of generating short descriptive sentences from images A novel representation intermediate between images and sentences A novel, discriminative approach">
<meta name="keywords" content="Image Caption,Paper Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="Every Picture Tells a Story:Generating Sentences from Images">
<meta property="og:url" content="http://yoursite.com/2017/01/16/Every-Picture-Tells-a-Story-Generating-Sentences-from-Images/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Main Contribution A dataset to study the problem of generating short descriptive sentences from images A novel representation intermediate between images and sentences A novel, discriminative approach">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/1484534108821.png">
<meta property="og:updated_time" content="2017-01-16T12:15:37.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Every Picture Tells a Story:Generating Sentences from Images">
<meta name="twitter:description" content="Main Contribution A dataset to study the problem of generating short descriptive sentences from images A novel representation intermediate between images and sentences A novel, discriminative approach">
<meta name="twitter:image" content="http://yoursite.com/images/1484534108821.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Every-Picture-Tells-a-Story-Generating-Sentences-from-Images" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/01/16/Every-Picture-Tells-a-Story-Generating-Sentences-from-Images/" class="article-date">
  <time datetime="2017-01-16T11:31:17.000Z" itemprop="datePublished">2017-01-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Every Picture Tells a Story:Generating Sentences from Images
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Main-Contribution"><a href="#Main-Contribution" class="headerlink" title="Main Contribution"></a>Main Contribution</h2><ol>
<li>A dataset to study the problem of generating short descriptive sentences from images</li>
<li>A novel representation intermediate between images and sentences</li>
<li>A novel, discriminative approach that produces very good results at sentence annotation</li>
<li>Methods to use distributional semantics to cope with out of vocabulary words for illustration</li>
<li>A quantitative evaluation of sentence generation at a useful scale<a id="more"></a>
</li>
</ol>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><ul>
<li>Assume a space of Meanings that comes between the space of Sentences and the space of Images</li>
<li>Map each to the meaning space</li>
<li>Compare the results</li>
</ul>
<h3 id="Mapping-Image-to-Meaning"><a href="#Mapping-Image-to-Meaning" class="headerlink" title="Mapping Image to Meaning"></a>Mapping Image to Meaning</h3><p>Representation of meaning: <strong>a triplet of &lt;object, action, scene&gt;</strong><br><strong>Why?</strong></p>
<blockquote>
<p>This triplet provides a holistic idea about what the image (resp. sentence) is about and what is most important.For the image, this is the part that people would talk about first; for the sentence, this is the structure that should be preserved in the tightest summary.</p>
</blockquote>
<p><strong>How to construct the triplet?</strong><br>Each slot in the meaning representation can take a value from a set of discrete values.The edges correspond to the binary relationships between nodes.<br><img src="\images\1484534108821.png" alt="Alt text|middle"><br>Objects: 23 nouns<br>Actions: 15 values<br>Scenes: 29 values</p>
<p><strong>How to do inference?</strong></p>
<blockquote>
<p>Having provided the potentials of the MRF, we use a greedy method to do inference. Inference involves finding the best selection of the discrete sets of values given the unary and binary potentials.</p>
</blockquote>
<h3 id="Image-Potentials"><a href="#Image-Potentials" class="headerlink" title="Image Potentials"></a>Image Potentials</h3><h4 id="Node-Potentials"><a href="#Node-Potentials" class="headerlink" title="Node Potentials"></a>Node Potentials</h4><p><strong>Image features</strong></p>
<ul>
<li>Felzenszwalb et al. detector responses</li>
<li>Hoiem et al. classification responses</li>
<li>Gist-based scene classification responses</li>
</ul>
<p><strong>Node features</strong><br>Node features are built by fitting a linear SVM to predict each of the nodes independently on the image features.</p>
<blockquote>
<p>This is a number-of-nodes-dimensional vector and each element in this vector provides a score for a node given the image.</p>
</blockquote>
<p>(TLDR)Similar images are expected to have similar meanings, so a set of note potentials are computed as a combination of node features :</p>
<blockquote>
<ul>
<li>by matching image features, we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the image side</em>. By doing so, we have a representation of what the node features are for similar images. </li>
<li>by matching image features, we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the sentence side.</em> By doing so, we have a representation of what the sentence representation are for images that look like our image.</li>
<li>by matching those node features derived from classifiers and detectors (above), we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the image side</em>. By doing so, we have a representation of what the node features are for images that produce similar classifier and detector outputs.</li>
<li>by matching those node features derived from classifiers and detectors (above), we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the sentence side</em>. By doing so, we have a representation of what the sentence representation does for images that produce similar classifier and detector outputs.</li>
</ul>
</blockquote>
<h4 id="Edge-Potentials"><a href="#Edge-Potentials" class="headerlink" title="Edge Potentials"></a>Edge Potentials</h4><p><strong>Two problems</strong></p>
<ol>
<li>Introducing a parameter for each edge results in unmanageable number of parameters</li>
<li>Estimates of the parameters for the majority of edges would be noisy</li>
</ol>
<p><strong>Solution</strong></p>
<ol>
<li>Control the number of parameters</li>
<li>Do smoothing</li>
</ol>
<p><strong>How to limit the number of parameters?</strong><br>Use four different estimates for edges:</p>
<ul>
<li>The normalized frequency of the word A in our corpus, $f(A)$</li>
<li>The normalized frequency of the word B in our corpus, $f(B)$</li>
<li>The normalized frequency of (A and B) at the same time, $f(A, B)$</li>
<li>$f(A, B)\over f(A)f(B)$</li>
</ul>
<h4 id="Sequence-Potentials"><a href="#Sequence-Potentials" class="headerlink" title="Sequence Potentials"></a>Sequence Potentials</h4><p><strong>How to represent a sentence?</strong><br>Curran &amp; Clark parser –&gt; dependency parse –&gt; (object, action) pairs</p>
<p><strong>How to measure similarity?</strong><br>Lin Similarity Measure</p>
<p><strong>How to discover co-occurring actions?</strong><br>Action Co-occurrence Score</p>
<p><strong>Node potentials</strong></p>
<blockquote>
<ol>
<li>First we compute the similarity of each object, scene, and action extracted from each sentence. This gives us the the first estimates for the potentials over the nodes. We call this the sentence node feature.</li>
<li>For each sentence, we also compute the average of sentence node features for other four sentences describing the same images in the train set.</li>
<li>We compute the average of k nearest neighbors in the sentence node features space for a given sentence. We consider this as our third estimate for nodes.</li>
<li>We also compute the average of the image node features for images corresponding to the nearest neighbors in the item above.</li>
<li>The average of the sentence node features of reference sentences for the nearest neighbors in the item 3 is considered as our fifth estimate for nodes.</li>
<li>We also include the sentence node feature for the reference sentence.</li>
</ol>
</blockquote>
<p><strong>Edge Potentials</strong><br>Identical to those of images.</p>
<h4 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h4><p>2 mappings to be learned:</p>
<ol>
<li>image space –&gt; meaning space, using image potentials</li>
<li>sentence space –&gt; meaning space, using sentence potentials</li>
</ol>
<p><strong>Structure learning problem</strong><br>$$\min_{\omega} {\lambda\over2}\lVert\omega\rVert^2+{1\over n}\sum _{i\in examples}\xi _i$$<br>subject to<br>$$\omega\Phi(x_i,y_i)+\xi_i\ge\max_{y\in meaning\ space }\omega\Phi(x_i,y)+L(y_i,y)\ \forall i\in examples$$<br>$$\xi_i\ge 0 \ \forall i \in examples$$</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>Built from PASCAL 2008 images with Amazon Mechanical Turk</p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/01/16/Every-Picture-Tells-a-Story-Generating-Sentences-from-Images/" data-id="cjhixtmro0004n61ri480a6s9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Image-Caption/">Image Caption</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/02/27/Paper-Excerpts-in-Image-Caption/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Paper Excerpts in Image Caption
        
      </div>
    </a>
  
  
    <a href="/2017/01/12/Show-and-Tell-Lessons-learned-from-the-2015-MSCOCO-Image-Captioning-Challenge/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hacking/">Hacking</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CTF/">CTF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Image-Caption/">Image Caption</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Object-Detection/">Object Detection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CTF/" style="font-size: 13.33px;">CTF</a> <a href="/tags/Image-Caption/" style="font-size: 16.67px;">Image Caption</a> <a href="/tags/Object-Detection/" style="font-size: 10px;">Object Detection</a> <a href="/tags/Paper-Notes/" style="font-size: 20px;">Paper Notes</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/05/23/test/">test</a>
          </li>
        
          <li>
            <a href="/2018/05/23/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2017/03/01/Faster-R-CNN/">Faster R-CNN</a>
          </li>
        
          <li>
            <a href="/2017/02/28/DenseCap-Fully-Convolutional-Localization-Networks-for-Dense-Captioning/">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</a>
          </li>
        
          <li>
            <a href="/2017/02/27/Paper-Excerpts-in-Image-Caption/">Paper Excerpts in Image Caption</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>