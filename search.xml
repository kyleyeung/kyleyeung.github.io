<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Where&#39;d You Go?</title>
      <link href="/2018/08/23/where-would-you-go/"/>
      <url>/2018/08/23/where-would-you-go/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Tupac, <a href="http://music.163.com/#/m/song?id=28837115" target="_blank" rel="noopener"><em>Where’d you go</em></a>:</p><blockquote><p>My main man had two strikes slipped</p><p>Got arrested and flipped</p><p>He screamed thug life and emptied the clip</p></blockquote><p>和友人吃饭，被问了这样一个问题：「你的朋友中现在混得最好的大概是什么样子？」一时没想明白，简单作答：大部分都还在读书。</p><p>后来又想起这个问题，发现这确是值得思考的视角。</p><p>什么是「混得好」？是指世俗意义上的「成功」吗？这里当然不会通过否定世俗的成功来自我标榜某种「独特」价值观，但人们的生活/精神状态与外部成功指标的关系，可以拿来聊一聊。</p><p>「朋友」们拥有怎样值得欣赏的共同特质？ 她们<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" aria-label="「她们」是对一群包含了不同性别的人的泛指，同「他们」。">[1]</span></a></sup>多半对如下问题的中两个以上有着清晰、明确且逻辑自洽的答案：</p><ol><li><p>现在在做什么？为什么？</p></li><li><p>将来想要做什么？为什么？</p></li><li><p>你喜欢什么？为什么？</p></li><li><p>自身的优势和局限在哪里？会怎么做？</p></li><li><p>是否在某一专业领域有较为长期的积累，并且有独到见解？</p></li></ol><p>或者更简单一些，直接问一个简短问题：「当你实现最大程度的财富自由后，你会去做什么？」她们的答案不会是「不知道」，也不会是「我要天天躺在家里数/花钱」。</p><p>按照字面意义的「成功」来作答，则会陷入一种不妙的境地。该如何向一个人证明达成某些指标就是「混得好」呢？人们习惯了用诸如「藤校精英」「投行新贵」「有力人士」「官居 X 品」「北京房姐」这样的标签来标榜，但只不过是男人的长度和女人的罩杯，当局者吹得很开心，明眼人却都知道不是那么回事儿。勃学与<a href="https://mp.weixin.qq.com/s/F6AHvTEcPPT0iS-3hpxGeg" target="_blank" rel="noopener">失败学</a>的出现和兴盛，说明一部分人对前述那套价值体系已失去信心。</p><p>有钱当然是件好事情，有更多可支配的资源总是好的，但精神文明建立于物质文明之上是一句错误的废话。</p><p>可以一边精神自由而一边世俗成功吗？只有祝各位成功地成功吧。</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「她们」是对一群包含了不同性别的人的泛指，同「他们」。<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      <categories>
          
          <category> Life Record </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>什么是计算社会科学？</title>
      <link href="/2018/06/21/what-is-computational-social-science/"/>
      <url>/2018/06/21/what-is-computational-social-science/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>计算社会科学是一个以计算为媒介，对社会科学问题进行研究的学科。</p><p>Claudio Cioffi-Revilla在其所著的<em>Introdcution to Computational Social Science: Principles and Appilcations</em>一书中，给出定义如下：</p><blockquote><p>The new field of <strong>Computaional Social Science</strong> can be defined as the interdisciplinary investigation of the social universe on many scales, ranging from individual actors to the largest groupings, through the medium of computation.</p></blockquote><p>这是一个较为宽泛但准确的定义。同时参考<em>Wikipedia</em>上Computational Social Science词条：</p><blockquote><p><strong>Computational social science</strong> refers to the academic sub-disciplines concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena.</p></blockquote><p>计算社会科学的主要研究领域：</p><ol><li><p>自动化社会信息提取（Automatd Social Information Extraction）</p><p>早期的社会科学家从广播、报纸或者其他出版物等来源收集数据，现在这些工作都可以通过计算化工具来完成（自然语言处理、语音识别技术等）。使用自动化计算方法提取出的社会信息有两个用处：</p><p>其一，从影响、活动或其他研究者感兴趣的维度来分析数据内容：例如对演讲、立法委员会证词或其他公开记录进行计算化内容分析，提取出关于领导人或其他政府官员的政治倾向信息。即，在有明确衡量标准的前提下，通过数据分析来得出结论。</p><p>其二，信息提取算法可被用来对存在于原始数据中，但难以通过人类手工过程发现的网络或其他结构进行建模：例如使用计算内容分析方法和文本挖掘技术，对法院卷宗及其他描述了与犯罪个体相关的个人、日期、位置、时间、属性的证据性法律文本进行分析，从而对犯罪组织及其非法活动建立模型。这更类似于模式识别（Pattern Recognition），即在不明确具体目标的情况下，通过计算化手段建立模式。</p></li><li><p>社会网络(Social Networks)</p><p>随着社交媒体（Facebook, Twitter等）的飞速发展，该领域近年来也随之热门起来。例如<a href="https://news.vice.com/en_us/article/d3xamx/journalists-and-trump-voters-live-in-separate-online-bubbles-mit-analysis-shows" target="_blank" rel="noopener">这项研究</a></p><p>实际上在Big Five领域中（现代人类学，经济学，政治科学，心理学和社会学）对网络的分析早已有之，社会网络分析同时也是CSS研究中唯一具有完备文本历史（well-documented history）的领域。社会网络分析不仅有其内在价值，同时也对其他计算社会科学领域的理论和研究起到了推动作用。</p></li><li><p>社会复杂性(Social Complexity)</p><p>计算社会科学的研究基石之一便是对社会复杂性的理解，同时该领域在帮助人们深入理解社会复杂性与文明之起源时发挥的作用也变得日益显著。计算社会科学的理论包含了很广泛的社会科学概念，比如decision-making、coalition theory等等；与此同时，例如non-equilibrium distributions, power laws这样与当代科学高度相关的理论也被包含在了计算社会科学对社会复杂性的研究之中。</p></li><li><p>社会模拟建模(Social Simulation Modeling)</p><p>社会科学领域的模拟建模开始于数十年前，而计算社会科学方向最早的该类方法是系统动力学模型（system dynamics models）。这种基于差分或微分方程的模型取决于条件和数据的需求。另一种主流传统模型是排队模型(queuing models)，其使用排队论和若干种概率分布来描述获得服务的实体（如顾客，患者，客人）、服务时长或者其他服务过程中的概率特性。当然，还有元胞自动机（cellular automata）这样面向对象的模型，基于agent的模型和演化计算模型（evolutionary computaion models）等等。</p></li></ol><p>至于该领域的著名研究者，可以参考09年Science上的<a href="https://gking.harvard.edu/files/LazPenAda09.pdf" target="_blank" rel="noopener">这篇文章</a>,顺着作者列表看一遍会比较清楚：比如MIT的Alex Pentland；必须要提及的还有Duncan watts,在该领域做了相当长时间的研究，youtube上可以找到不少他的讲座；同时，Jennifer Pan也是目前该领域的学术新星，但她隶属于斯坦福Department of Communication，这也体现出该方向的研究者实际上大多分布于各个相关领域之中，有独立CSS department的大学和机构目前并不多见。</p>]]></content>
      
      <categories>
          
          <category> Computational Social Science </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>生日快乐,凯尔先生</title>
      <link href="/2018/06/18/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90%EF%BC%8C%E5%87%AF%E5%B0%94%E5%85%88%E7%94%9F2018/"/>
      <url>/2018/06/18/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90%EF%BC%8C%E5%87%AF%E5%B0%94%E5%85%88%E7%94%9F2018/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>25岁，人生过去了五分之一。毕竟21世纪了，预期寿命125岁没什么问题。</p><p>2014年我开始每天写日记，在生日这天回顾过去一年的经历，不想在庸庸碌碌中把自己活丢了。当回头把这些记录串联起来时发现，尽管没有刻意设计，不经意间还是活出了一个连贯故事。脑中的「自我」是不准确的，因为夹杂了对未来的强烈期许，和对过去某些遗憾的强迫反复。唯有自己笔下和他人眼中的形象，更可能接近客观。</p><p>24岁这一年，我做了人生中第二个重要决定，选择离开体制重新开始，从南京到北京。完成了硕士论文，尽管导师和自己都不甚满意。膝盖受了伤，怕是从此与长跑和足球都已无缘。365日里每一天都倍感漫长，但说起来，决定了日后大多数能与不能的，就是那几个关键事件而已。有时候你可以选，有时候不知不觉就在某条路上走了很远。</p><p>这一年，丢掉不喜欢的，也失去了爱过的。其实伏笔早已埋下：如果当初没有考研，就不会选择现在的研究方向，更不会因为热爱而毅然从体制脱身；如果更早地了解到科学训练和伤病预防的方法，也不会在好几年里都和跑步这件事纠缠，不会以伤病告终。没有这两件事，也就没有再往后的一系列连锁反应。这样说起来，还是知识改变一切。</p><p>1月31日来到五道口开始实习，四舍五入接近半年。绝大部分时间都在忙碌，在高速运转，在思考未来。偶尔那么一两次会突然回过神来：这个选择是对的吗？念头升起的下一瞬间就哑然失笑，因为问题一开始就不存在。有的只是迷茫疲惫时懒惰小人的暂时领先，和心底安稳本能的短命复辟。吃顿饭，洗个澡，睡一觉，一套舒适三连能解决大多数问题。</p><ol><li><p>现在的生活开心吗？</p><p>开心。尽管也有每天都会担忧的事情。</p></li><li><p>最高兴的是什么?</p><p>还年轻，还有机会。但时间正逐渐减少。不用考虑婚姻，不用过多担心物质现实，不确定这样的自由还能持续多久。</p></li><li><p>最担心的是什么？</p><p>对未来有大致的方向，但在两条路途之间的选择上暂时没有下定决心，还没彻底想明白。</p></li><li><p>最缺乏的是什么？</p><p>知识。也会希望能有多一些钱用来支撑知识的获取。</p></li><li><p>假设衣食富足，最想做的是什么？为什么？</p><p>去读Phd。因为肾上腺素。</p></li><li><p>目前为止最后悔的一件事是什么？为什么？</p><p>浪费过太多时间。</p></li><li><p>收入如何？</p><p>够吃够喝，勉强养活自己。每个月能去酒吧喝一两杯，又或者看一场演出。买书倒是不用顾忌。没房没车。</p></li><li><p>最近在读什么书？</p><p><em>Neuromancer</em></p></li><li><p>对自己最不满意的一点？</p><p>没有在任何一件事上持续投入3年以上的努力。找不到特别值得骄傲的地方。</p></li><li><p>健康状况如何？</p><p>最近有些缺乏睡眠。膝伤似乎加重了，一直没敢去跑步，踢球只能想一想，康复训练断断续续。还在长痘。</p></li><li><p>最骄傲的一点？</p><p>皮糙能抗。</p></li><li><p>其他还有什么想说的？</p><p>我发现自己目前所取得的成就，与运气的相关程度大于与努力的相关程度。这几年遇到了非常多的前辈、老师、朋友，在不同的时刻给了我数不胜数的指引与帮助。倘若没有这些加持，只靠努力怕是远远不够的。</p><p>如果说有什么是现在就可以认定的原则，那就是一定要选心中最爱的那个，一想起来就心跳加速肾上腺素疯狂分泌的那个，无论是事业还是爱情还是其他。顺遂心意最重要。</p></li></ol><p>希望精力更充沛一些，能做更多的事而不被睡眠不足困扰；<br>希望身体更强健一些，能支撑自己探索更大更精彩的世界；<br>希望头脑更聪明一些，追求知识时不再费尽心思不得其解。</p><p>All life is problem solving. 永远年轻，永远热泪盈眶。</p><p>附上以前的生日文章：<br><a href="http://kyleyang.net/2014/06/17/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90%EF%BC%8C%E5%87%AF%E5%B0%94%E5%85%88%E7%94%9F2014/" target="_blank" rel="noopener">生日快乐，凯尔先生  2014</a><br><a href="http://kyleyang.net/2017/06/17/%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90%EF%BC%8C%E5%87%AF%E5%B0%94%E5%85%88%E7%94%9F2017/" target="_blank" rel="noopener">生日快乐，凯尔先生  2017</a></p>]]></content>
      
      <categories>
          
          <category> Life Record </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Birthday </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Curbing My Enthusiasm</title>
      <link href="/2018/05/23/Curbing-my-enthusiasm/"/>
      <url>/2018/05/23/Curbing-my-enthusiasm/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>“The scope of thoughts you can write is much broader than the scope of thoughts you can think on your own. Written language, much like computer code, can express programs that are out of reach for brains alone. Write to think.” — <strong>François Chollet</strong> <a href="https://twitter.com/fchollet" target="_blank" rel="noopener">@fchollet</a></p></blockquote><p>After more than a whole year, I’m finally bringing back this website online. Inspired by <a href="https://blog.yitianshijie.net/2017/08/13/yitianshijie-newsletter-78/" target="_blank" rel="noopener">this</a>,  writings here will not only be academic notes this time.</p><p>Instead of randomly throwing a few words accompanied by some funny pics on twitter/wechat and totally dropping the ideas afterwards, writing and posting here is expected to give birth to deeper and wider thoughts.</p><p>Hence, I’m practicing to curb my enthusiasm.</p>]]></content>
      
      <categories>
          
          <category> Life Record </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Faster R-CNN</title>
      <link href="/2017/03/01/Faster-R-CNN/"/>
      <url>/2017/03/01/Faster-R-CNN/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="main-contribution"><a class="header-anchor" href="#main-contribution">¶</a>Main Contribution</h2><p><strong>Region Proposal Network (RPN)</strong></p><ul><li>Nearly cost-free proposals</li></ul><p><em>To be continued…</em></p><a id="more"></a>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper Notes </tag>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DenseCap: Fully Convolutional Localization Networks for Dense Captioning</title>
      <link href="/2017/02/28/DenseCap-Fully-Convolutional-Localization-Networks-for-Dense-Captioning/"/>
      <url>/2017/02/28/DenseCap-Fully-Convolutional-Localization-Networks-for-Dense-Captioning/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="main-contribution"><a class="header-anchor" href="#main-contribution">¶</a>Main Contribution</h2><ol><li>Introduced dense captioning task</li><li>Proposed Fully Convolutional Localization Network</li><li>Evaluation on Visual  Genome dataset<br><img src=".%5Cimages%5C1487395538664.png" alt="Alt text"></li></ol><a id="more"></a><h2 id="related-work"><a class="header-anchor" href="#related-work">¶</a>Related Work</h2><h3 id="object-detection"><a class="header-anchor" href="#object-detection">¶</a>Object Detection</h3><ul><li>CNN</li><li>R-CNN : each region of interest was processed independently</li><li>Fast R-CNN : processing all regions with only single forward pass of the CNN</li><li><strong>Faster R-CNN</strong>:a region proposal network (RPN) that regresses from anchors to regions of interest</li></ul><p>Difference from Faster R-CNN:</p><blockquote><p>However, they adopt a 4-step optimization process, while our approach does not require training pipelines. Additionally, we replace their RoI pooling mechanism with a differentiable, spatial soft attention mechanism. In particular, this change allows us to backpropagate through the region proposal network and train the whole model jointly.</p></blockquote><h3 id="image-caption"><a class="header-anchor" href="#image-caption">¶</a>Image Caption</h3><ul><li><strong>Deep visual-semantic alignments for generating image descriptions</strong></li></ul><p>Difference:</p><blockquote><p>…but they do not tackle the joint task of detection of description in one model. Our model is end-to-end and designed in such way that the prediction for each region is a function of the global   image context, which we show also ultimately leads to stronger performance.</p></blockquote><h2 id="model"><a class="header-anchor" href="#model">¶</a>Model</h2><h3 id="overview"><a class="header-anchor" href="#overview">¶</a>Overview</h3><p>To design an architecture that jointly localizes regions of interest and then describes each with natural language.</p><h3 id="model-atchitecture"><a class="header-anchor" href="#model-atchitecture">¶</a>Model Atchitecture</h3><h4 id="cnn"><a class="header-anchor" href="#cnn">¶</a>CNN</h4><p>VGG-16, remove the final pooling layer</p><h4 id="fully-convolutional-localization-layer"><a class="header-anchor" href="#fully-convolutional-localization-layer">¶</a>Fully Convolutional Localization Layer</h4><p>Based on that of Faster R-CNN with a few improvements.</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Caption </tag>
            
            <tag> Paper Notes </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Paper Excerpts in Image Caption</title>
      <link href="/2017/02/27/Paper-Excerpts-in-Image-Caption/"/>
      <url>/2017/02/27/Paper-Excerpts-in-Image-Caption/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>This article intends to become a collection of brief summaries of papers in recent years (2014-), in the area of deep image captioning.</p><a id="more"></a><h2 id="2014"><a class="header-anchor" href="#2014">¶</a>2014</h2><hr><h3 id="deep-fragment-embeddings-for-bidirectional-image-sentence-mapping"><a class="header-anchor" href="#deep-fragment-embeddings-for-bidirectional-image-sentence-mapping">¶</a>Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</h3><p>arXiv:1406.5679</p><h4 id="model"><a class="header-anchor" href="#model">¶</a>Model</h4><p>A model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data.<br>The model works on a finer level and embeds <strong>fragments of images</strong> (objects) and <strong>fragments of sentences</strong> (typed dependency tree relations) into a common space.<br>New <strong>fragment alignment objective</strong> that learns to directly associate these fragments across modalities.</p><h4 id="representation"><a class="header-anchor" href="#representation">¶</a>Representation</h4><p>Image: RCNN detections -&gt; Image Fragments<br>Sentence: Dependency relations -&gt; Sentence fragments</p><h4 id="experiment"><a class="header-anchor" href="#experiment">¶</a>Experiment</h4><p>Sentence/Image Retrieval</p><h3 id="multimodal-neural-language-models"><a class="header-anchor" href="#multimodal-neural-language-models">¶</a>Multimodal Neural Language Models</h3><p>ICML 2014<br><em>This paper is outdated, no need for further research.</em><br>####Model<br>Multimodal Log-Bilinear Models</p><h3 id="explaining-images-with-multimodal-recurrent-neutal-networks"><a class="header-anchor" href="#explaining-images-with-multimodal-recurrent-neutal-networks">¶</a>Explaining Images with Multimodal Recurrent Neutal Networks</h3><p>arXiv:1410.1090</p><h4 id="model-v2"><a class="header-anchor" href="#model-v2">¶</a>Model</h4><p><strong>m-RNN model</strong><br>Objective: the probability distribution of generating a word given previous words and the image<br>Image features are input at <strong>each</strong> word generation</p><h4 id="representation-v2"><a class="header-anchor" href="#representation-v2">¶</a>Representation</h4><p>Image features are extracted by a CNN, and then fed to m-RNN to generate the sentence.</p><h4 id="experiment-v2"><a class="header-anchor" href="#experiment-v2">¶</a>Experiment</h4><p>IAPR TC-12<br>Flickr 8K<br>Flickr 30K</p><h3 id="unifying-visual-semantic-embeddings-with-multimodal-neural-language-models"><a class="header-anchor" href="#unifying-visual-semantic-embeddings-with-multimodal-neural-language-models">¶</a>Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</h3><p>arXiv:1411.2539</p><h4 id="model-v3"><a class="header-anchor" href="#model-v3">¶</a>Model</h4><p>encoder-decoder models<br>For the encoder, we learn a joint image-sentence embedding where sentences are encoded using long short-term memory (LSTM) recurrent neural networks [1]. Image features from a deep convolutional network are projected into the embedding space of the LSTM hidden states.<br>For decoding, we introduce a new neural language model called the structure-content neural language model (SC-NLM).<br>Loss:pairwise ranking loss</p><h4 id="experiment-v3"><a class="header-anchor" href="#experiment-v3">¶</a>Experiment</h4><p>Image annotation/search: better than m—RNN</p><h3 id="learning-a-recurrent-visual-representation-for-image-caption-generation"><a class="header-anchor" href="#learning-a-recurrent-visual-representation-for-image-caption-generation">¶</a>Learning a Recurrent Visual Representation for Image Caption Generation</h3><p>arXiv:1411.5654 (and CVPR 2015)</p><h4 id="model-v4"><a class="header-anchor" href="#model-v4">¶</a>Model</h4><p>Use an RNN to learn <strong>bi-directional</strong> mapping between images and descriptions.<br>An <strong>Recurrent Visual Memory</strong> that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction.</p><h4 id="representation-v3"><a class="header-anchor" href="#representation-v3">¶</a>Representation</h4><p>A bi-directional representation capable of generating both novel descriptions from images and visual epresentations from descriptions.<br>As a word is generated or read the visual representation is updated to reflect the new information contained in the word.</p><h4 id="experiment-v4"><a class="header-anchor" href="#experiment-v4">¶</a>Experiment</h4><p>PASCAL 1K<br>Flickr 8k &amp; 30K<br>MS COCO<br>Caption: Perplexity BLEU METEOR<br>Sentence/Image Retrieval</p><h2 id="2015"><a class="header-anchor" href="#2015">¶</a>2015</h2><hr><h3 id="long-term-recurrent-convolutional-networks-for-visual-recognition-and-description"><a class="header-anchor" href="#long-term-recurrent-convolutional-networks-for-visual-recognition-and-description">¶</a>Long-term Recurrent Convolutional Networks for Visual Recognition and Description</h3><p>CVPR 2015</p><h4 id="model-v5"><a class="header-anchor" href="#model-v5">¶</a>Model</h4><p>LRCN(Longterm Recurrent Convulutional Network)<br>Different from CNN-RNN model, it used a CNN and a stack of 4 LSTMs for image caption.<br>Image features are provided as input to the sequential modal at <strong>each</strong> timestep.</p><h4 id="experiment-v5"><a class="header-anchor" href="#experiment-v5">¶</a>Experiment</h4><p>Flickr 30k<br>MS COCO 2014<br>better than m-RNN model （B1 58~67）</p><h3 id="from-captions-to-visual-concepts-and-back"><a class="header-anchor" href="#from-captions-to-visual-concepts-and-back">¶</a>From Captions to Visual Concepts and Back</h3><p>CVPR 2015</p><h4 id="model-v6"><a class="header-anchor" href="#model-v6">¶</a>Model</h4><p>Pipeline:<br>detect words -&gt; generate sentences -&gt; re-rank sentences</p><ol><li>Use weakly-supervised learning to create detectors for a set of words commonly found in image captions</li><li>Train a maximum entropy (ME) LM from a set of training image descriptions</li><li>Re-rank a set of high-likelihood sentences by a linear weighting of sentence features</li></ol><h4 id="experiment-v6"><a class="header-anchor" href="#experiment-v6">¶</a>Experiment</h4><p>MSCOCO</p><h3 id="image-specificity"><a class="header-anchor" href="#image-specificity">¶</a>Image Specificity</h3><h4 id="contribution"><a class="header-anchor" href="#contribution">¶</a>Contribution</h4><p>Introduced the notion of image specificity.<br>Presented two mechanisms to measure specificity given multiple descriptions of an image: an automated measure and a measure that relies on human judgement.</p><h4 id="insight"><a class="header-anchor" href="#insight">¶</a>Insight</h4><p>The rationale is the following: a specific image should be ranked high only if the query description matches the reference description of that image well, because we know that sentences that describe this image tend to be very similar. For ambiguous images, on the other hand, even mediocre similarities between query and reference descriptions may be good enough.</p><h4 id="experiment-v7"><a class="header-anchor" href="#experiment-v7">¶</a>Experiment</h4><p>Image Search</p><h2 id="2016"><a class="header-anchor" href="#2016">¶</a>2016</h2><hr><h3 id="densecap-fully-convolutional-localization-networks-for-dense-captioning"><a class="header-anchor" href="#densecap-fully-convolutional-localization-networks-for-dense-captioning">¶</a>DenseCap: Fully Convolutional Localization Networks for Dense Captioning</h3><p>CVPR 2016</p><h2 id="2017"><a class="header-anchor" href="#2017">¶</a>2017</h2><hr><h3 id="mat-a-multimodal-attentive-translator-for-image-captioning"><a class="header-anchor" href="#mat-a-multimodal-attentive-translator-for-image-captioning">¶</a>MAT: A Multimodal Attentive Translator for Image Captioning</h3><p>arXiv:1702.05658</p><h4 id="model-v7"><a class="header-anchor" href="#model-v7">¶</a>Model</h4><p>a <strong>sequence-to-sequence RNN model</strong><br>The input image is represented as <strong>a sequence of detected objects</strong> which feeds as the source sequence of the RNN model.</p><h4 id="representation-v4"><a class="header-anchor" href="#representation-v4">¶</a>Representation</h4><p>extract the objects features in the image and arrange them in a order using convolutional neural networks.<br>a sequential attention layer is introduced to selectively attend to the objects that are related to generate corresponding words in the sentences.</p><h4 id="experiment-v8"><a class="header-anchor" href="#experiment-v8">¶</a>Experiment</h4><p>surpasses the state-of-the-art methods in all metrics<br>a CIDEr of 1.029 (c5) and 1.064 (c40) on MSCOCO evaluation server</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Caption </tag>
            
            <tag> Paper Notes </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Every Picture Tells a Story:Generating Sentences From Images</title>
      <link href="/2017/01/16/Every-Picture-Tells-a-Story-Generating-Sentences-from-Images/"/>
      <url>/2017/01/16/Every-Picture-Tells-a-Story-Generating-Sentences-from-Images/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="main-contribution"><a class="header-anchor" href="#main-contribution">¶</a>Main Contribution</h2><ol><li>A dataset to study the problem of generating short descriptive sentences from images</li><li>A novel representation intermediate between images and sentences</li><li>A novel, discriminative approach that produces very good results at sentence annotation</li><li>Methods to use distributional semantics to cope with out of vocabulary words for illustration</li><li>A quantitative evaluation of sentence generation at a useful scale</li></ol><a id="more"></a><h2 id="approach"><a class="header-anchor" href="#approach">¶</a>Approach</h2><ul><li>Assume a space of Meanings that comes between the space of Sentences and the space of Images</li><li>Map each to the meaning space</li><li>Compare the results</li></ul><h3 id="mapping-image-to-meaning"><a class="header-anchor" href="#mapping-image-to-meaning">¶</a>Mapping Image to Meaning</h3><p>Representation of meaning: <strong>a triplet of &lt;object, action, scene&gt;</strong><br><strong>Why?</strong></p><blockquote><p>This triplet provides a holistic idea about what the image (resp. sentence) is about and what is most important.For the image, this is the part that people would talk about first; for the sentence, this is the structure that should be preserved in the tightest summary.</p></blockquote><p><strong>How to construct the triplet?</strong><br>Each slot in the meaning representation can take a value from a set of discrete values.The edges correspond to the binary relationships between nodes.<br><img src="%5Cimages%5C1484534108821.png" alt="Alt text|middle"><br>Objects: 23 nouns<br>Actions: 15 values<br>Scenes: 29 values</p><p><strong>How to do inference?</strong></p><blockquote><p>Having provided the potentials of the MRF, we use a greedy method to do inference. Inference involves finding the best selection of the discrete sets of values given the unary and binary potentials.</p></blockquote><h3 id="image-potentials"><a class="header-anchor" href="#image-potentials">¶</a>Image Potentials</h3><h4 id="node-potentials"><a class="header-anchor" href="#node-potentials">¶</a>Node Potentials</h4><p><strong>Image features</strong></p><ul><li>Felzenszwalb et al. detector responses</li><li>Hoiem et al. classification responses</li><li>Gist-based scene classification responses</li></ul><p><strong>Node features</strong><br>Node features are built by fitting a linear SVM to predict each of the nodes independently on the image features.</p><blockquote><p>This is a number-of-nodes-dimensional vector and each element in this vector provides a score for a node given the image.</p></blockquote><p>(TLDR)Similar images are expected to have similar meanings, so a set of note potentials are computed as a combination of node features :</p><blockquote><ul><li>by matching image features, we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the image side</em>. By doing so, we have a representation of what the node features are for similar images.</li><li>by matching image features, we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the sentence side.</em> By doing so, we have a representation of what the sentence representation are for images that look like our image.</li><li>by matching those node features derived from classifiers and detectors (above), we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the image side</em>. By doing so, we have a representation of what the node features are for images that produce similar classifier and detector outputs.</li><li>by matching those node features derived from classifiers and detectors (above), we obtain the k-nearest neighbours in the training set to the test image, then compute the average of the node features over those neighbours, <em>computed from the sentence side</em>. By doing so, we have a representation of what the sentence representation does for images that produce similar classifier and detector outputs.</li></ul></blockquote><h4 id="edge-potentials"><a class="header-anchor" href="#edge-potentials">¶</a>Edge Potentials</h4><p><strong>Two problems</strong></p><ol><li>Introducing a parameter for each edge results in unmanageable number of parameters</li><li>Estimates of the parameters for the majority of edges would be noisy</li></ol><p><strong>Solution</strong></p><ol><li>Control the number of parameters</li><li>Do smoothing</li></ol><p><strong>How to limit the number of parameters?</strong><br>Use four different estimates for edges:</p><ul><li>The normalized frequency of the word A in our corpus, $f(A)$</li><li>The normalized frequency of the word B in our corpus, $f(B)$</li><li>The normalized frequency of (A and B) at the same time, $f(A, B)$</li><li>$f(A, B)\over f(A)f(B)$</li></ul><h4 id="sequence-potentials"><a class="header-anchor" href="#sequence-potentials">¶</a>Sequence Potentials</h4><p><strong>How to represent a sentence?</strong><br>Curran &amp; Clark parser --&gt; dependency parse --&gt; (object, action) pairs</p><p><strong>How to measure similarity?</strong><br>Lin Similarity Measure</p><p><strong>How to discover co-occurring actions?</strong><br>Action Co-occurrence Score</p><p><strong>Node potentials</strong></p><blockquote><ol><li>First we compute the similarity of each object, scene, and action extracted from each sentence. This gives us the the first estimates for the potentials over the nodes. We call this the sentence node feature.</li><li>For each sentence, we also compute the average of sentence node features for other four sentences describing the same images in the train set.</li><li>We compute the average of k nearest neighbors in the sentence node features space for a given sentence. We consider this as our third estimate for nodes.</li><li>We also compute the average of the image node features for images corresponding to the nearest neighbors in the item above.</li><li>The average of the sentence node features of reference sentences for the nearest neighbors in the item 3 is considered as our fifth estimate for nodes.</li><li>We also include the sentence node feature for the reference sentence.</li></ol></blockquote><p><strong>Edge Potentials</strong><br>Identical to those of images.</p><h4 id="learning"><a class="header-anchor" href="#learning">¶</a>Learning</h4><p>2 mappings to be learned:</p><ol><li>image space --&gt; meaning space, using image potentials</li><li>sentence space --&gt; meaning space, using sentence potentials</li></ol><p><strong>Structure learning problem</strong><br>$$\min_{\omega} {\lambda\over2}\lVert\omega\rVert^2+{1\over n}\sum _{i\in examples}\xi <em>i$$<br>subject to<br>$$\omega\Phi(x_i,y_i)+\xi_i\ge\max</em>{y\in meaning\ space }\omega\Phi(x_i,y)+L(y_i,y)\ \forall i\in examples$$<br>$$\xi_i\ge 0 \ \forall i \in examples$$</p><h2 id="evaluation"><a class="header-anchor" href="#evaluation">¶</a>Evaluation</h2><h3 id="dataset"><a class="header-anchor" href="#dataset">¶</a>Dataset</h3><p>Built from PASCAL 2008 images with Amazon Mechanical Turk</p><h3 id="inference"><a class="header-anchor" href="#inference">¶</a>Inference</h3>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Caption </tag>
            
            <tag> Paper Notes </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Show and Tell: Lessons Learned From the 2015 MSCOCO Image Captioning Challenge</title>
      <link href="/2017/01/12/Show-and-Tell-Lessons-learned-from-the-2015-MSCOCO-Image-Captioning-Challenge/"/>
      <url>/2017/01/12/Show-and-Tell-Lessons-learned-from-the-2015-MSCOCO-Image-Captioning-Challenge/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="main-contribution"><a class="header-anchor" href="#main-contribution">¶</a>Main contribution</h2><ol><li>An end-to-end system for image caption</li><li>The model combines state-of-art sub-networks for vision and language models, yielding significantly better performance compared to state-of-the-art approaches</li></ol><ul><li>On the Pascal dataset, NIC yielded a BLEU score of 59, to be compared to the current state-of-the-art of 25, while human performance reaches 69</li><li>On Flickr30k, we improve from 56 to 66, and on SBU, from 19 to 28</li></ul><ol start="3"><li>We describe the lessons learned from participating in the first MSCOCO competition, which helped us to improve our initial model and place first in automatic metrics, and first (tied with another team) in human evaluation.</li></ol><a id="more"></a><h2 id="differences-from-related-work"><a class="header-anchor" href="#differences-from-related-work">¶</a>Differences from related work</h2><h3 id="from-mao-et-al"><a class="header-anchor" href="#from-mao-et-al">¶</a>from Mao et al.</h3><p><em>Explain images with multimodal recurrent neural networks</em><br><em>Deep captioning with multimodal recurrent neural networks (m-rnn)</em></p><ol><li>We use a more powerful RNN model</li><li>We provide the visual input to the RNN model directly</li><li>We have substantially better results on the established benchmarks</li></ol><h3 id="from-kiros-et-al"><a class="header-anchor" href="#from-kiros-et-al">¶</a>from Kiros et al.</h3><p><em>Unifying visual-semantic embeddings with multimodal neural language models</em></p><ol><li>They use two separate pathways (one for images, one for text) to define a joint embedding</li><li>Even though they can generate text, their approach is highly tuned for ranking</li></ol><h2 id="model"><a class="header-anchor" href="#model">¶</a>Model</h2><p>$$\theta^*=\arg\max_ {\theta}\sum_{(I,S)}logp(S|I;\theta)$$<br>$\theta$ are the parameters of our model, $I$ is an image, and $S$ its correct transcription</p><p>Since $S$ represents a sentence and its length is unbounded, so the chain rule is applied:<br>$$logp(S|I)=\sum_{t=0}^{N}logp(S_t|I,S_0,\cdots,S_{t-1})$$<br>A RNN is used to model $$p(S_t|I,S_0,\cdots,S_{t-1} )$$:<br>$$h_{t+1}=f(h_t,x_t)$$<br>Here, two <strong>crucial design choices</strong> are to be made:</p><ol><li>the form of $f$ :<strong>LSTM</strong></li><li>how are the images and words fed as inputs $x_t$:<ul><li>images -&gt; CNN (BatchNorm)</li><li>words -&gt; embedding from <em>Efficient estimation of word representations in vector space</em></li></ul></li></ol><h3 id="lstm-based-sentence-generator"><a class="header-anchor" href="#lstm-based-sentence-generator">¶</a>LSTM-based Sentence Generator</h3><p>(Introduction for LSTM ommitted)</p><h4 id="training"><a class="header-anchor" href="#training">¶</a>Training</h4><p>$$x_{-1}=CNN(I)$$<br>$$x_t=W_eS_t,t\in{ {0 \dots N-1} }$$<br>$$p_{t+1} = LSTM(x_t),t\in{ {0 \dots N-1} }$$<br>$S_t$ is a one-hot vector to the size of the dictionary, representing each word.<br>$S_0$ is a special start word and $S_N$ is a special stop word.<br>The image $I$ only input <strong>once</strong> at $t=-1$.Why <strong>once</strong>?</p><blockquote><p>We empirically verified that feeding the image at each time step as an extra input yields inferior results, as the network can explicitly exploit noise in the image and overfits more easily.</p></blockquote><p><strong>Loss Function</strong>:<br>$$L(I,S)=-\sum_{t=1}^Nlogp_t(S_t)$$<br>The above loss is minimized w.r.t. all the parameters of the LSTM, the top layer of the image embedder CNN and word embeddings $W_e$.</p><h4 id="inference"><a class="header-anchor" href="#inference">¶</a>Inference</h4><p>Two approches can be used  to generate a sentence given a image:</p><ol><li><strong>Sampling</strong><br>Sample the first word according to $p_1$, then continue sampling until the end token of some maximum length.</li><li><strong>BeamSearch</strong><br>Iteratively consider the set of the $k$ best sentences up to time $t$ as candidates to generate sentences of size $t+1$, and keep only the resulting best $k$ of them.<br>Why it’s better?<br>This better approximates $S=\arg\max_ {S<sup>\prime}p(S</sup>\prime|I)$</li></ol><h2 id="experiments"><a class="header-anchor" href="#experiments">¶</a>Experiments</h2><h3 id="evaluation"><a class="header-anchor" href="#evaluation">¶</a>Evaluation</h3><ol><li><strong>Human raters</strong></li></ol><ul><li>Ask the graders to evaluate each generated sentence with a scale from 1 to 4.</li><li>Each image was rated by 2 workers.</li><li>In case of disagreement we simply average the scores and record the average as the score</li></ul><ol start="2"><li><strong>BLEU</strong></li></ol><ul><li>A form of precision of word n-grams between generated and reference sentences</li><li>Correlates well with human evaluations</li></ul><ol start="3"><li><strong>Perplexity</strong></li></ol><ul><li>The geometric mean of the inverse probability for each predicted word</li><li>Not reported because BLEU is always preferred.</li></ul><ol start="4"><li><strong>CIDER</strong></li></ol><ul><li>It measures consistency between n-gram occurrences in generated and reference sentences, where this consistency is weighted by n-gram saliency and rarity.</li></ul><ol start="5"><li><strong>METEOR abd ROUGE</strong></li><li><strong>reacall@K</strong></li></ol><h3 id="datasets"><a class="header-anchor" href="#datasets">¶</a>Datasets</h3><table><thead><tr><th style="text-align:left">Dataset Name</th><th style="text-align:center">train</th><th style="text-align:center">valid</th><th style="text-align:center">test</th></tr></thead><tbody><tr><td style="text-align:left">Pascal VOC 2008</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">1000</td></tr><tr><td style="text-align:left">Flickr8k</td><td style="text-align:center">6000</td><td style="text-align:center">1000</td><td style="text-align:center">1000</td></tr><tr><td style="text-align:left">Flickr30k</td><td style="text-align:center">28000</td><td style="text-align:center">1000</td><td style="text-align:center">1000</td></tr><tr><td style="text-align:left">MSCOCO</td><td style="text-align:center">82783</td><td style="text-align:center">40504</td><td style="text-align:center">40775</td></tr><tr><td style="text-align:left">SBU</td><td style="text-align:center">1M</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table><blockquote><p>With the exception of SBU, each image has been annotated by labelers with 5 sentences that are relatively visual and unbiased. SBU consists of descriptions given by image owners when they uploaded them to Flickr. As such they are not guaranteed to be visual or unbiased and thus this dataset has more noise.</p></blockquote><ul><li><strong>Pascal dataset</strong> :customary used for testing only after a system has been trained on different data.</li><li><strong>SBU</strong>:1000 images were hold out for testing and train on the rest.</li><li><strong>MSCOCO</strong>: 4K random images reserved from the MSCOCO validation set as test, called COCO-4k, and are used to report results.</li></ul><h3 id="results"><a class="header-anchor" href="#results">¶</a>Results</h3><h4 id="training-details"><a class="header-anchor" href="#training-details">¶</a>Training Details</h4><p>How dataset size affects generalization?</p><blockquote><p>We believe that, even with the results we obtained which are quite good, the advantage of our method versus most current human-engineered approaches will only increase in the next few years as training set sizes will grow.</p></blockquote><p>How to deal with overfitting?</p><ol><li>Initialize the weights of the CNN component to a pretrained model (e.g., on ImageNet) -&gt; helped quite a lot</li><li>Initializing word embedding $W_e$ from a large news corpus -&gt; no significant gains were observed</li><li>Model level overfitting-avoiding techniques</li></ol><ul><li>Dropout -&gt; a few BLEU points improvement</li><li>Ensembling models -&gt; a few BLEU points improvement</li><li>Trading off number of hidden units versus depth</li></ul><p>How to train the weights?</p><ul><li>All sets of weights</li><li>Stochastic gradient descent</li><li>Fixed learning rate</li><li>No momentum</li><li>All weights were randomly initialized except for the CNN weights</li></ul><h4 id="generation-results"><a class="header-anchor" href="#generation-results">¶</a>Generation Results</h4><blockquote><p>We do think it is more meaningful to report BLEU-4, which is the standard in machine translation moving forward.</p></blockquote><h4 id="transfer-learning-data-size-and-label-quality"><a class="header-anchor" href="#transfer-learning-data-size-and-label-quality">¶</a>Transfer Learning, Data Size and Label Quality</h4><ul><li>Flickr30k to Flickr8k :the results obtained are 4 BLEU points better. <strong>Why?</strong> More training data added.</li><li>MSCOCO to Flicker8k:All the BLEU scores degrade by 10 points. <strong>Why?</strong> There are likely more differences in vocabulary and a larger mismatch.</li><li>MSCOCO to PASCAL: BLEU-1 59</li><li>Flicker30k to PASCAL: BLEU-1 53</li><li>MSCOCO to SBU: BLEU-1 degrades from 28. <strong>Why?</strong> SBU has weak labeling (i.e., the labels were captions and not human generated descriptions), the task is much harder with a much larger and noisier vocabulary.</li></ul><h4 id="generation-diversity-discussion"><a class="header-anchor" href="#generation-diversity-discussion">¶</a>Generation Diversity Discussion</h4><blockquote><p>If we take the best candidate, the sentence is present in the training set 80% of the times.</p></blockquote><p><strong>Why?</strong> The amount of training data is quite small, so it is relatively easy for the model to pick “exemplar” sentences and use them to generate descriptions.</p><blockquote><p>If we instead analyze the top 15 generated sentences, about half of the times we see a completely novel description, but still with a similar BLEU score, indicating that they are of enough quality, yet they provide a healthy diversity.</p></blockquote><h4 id="ranking-results"><a class="header-anchor" href="#ranking-results">¶</a>Ranking Results</h4><blockquote><p>NIC is doing surprisingly well on both ranking tasks.</p></blockquote><h4 id="human-evaluation"><a class="header-anchor" href="#human-evaluation">¶</a>Human Evaluation</h4><blockquote><p>BLEU is not a perfect metric, as it does not capture well the difference between NIC and human descriptions assessed by raters.</p></blockquote><h4 id="analysis-of-embeddings"><a class="header-anchor" href="#analysis-of-embeddings">¶</a>Analysis of Embeddings</h4><p>Some of the relationships learned by the model will help the vision component:</p><blockquote><p>Indeed, having “horse”, “pony”, and “donkey” close to each other will encourage the CNN to extract features that are relevant to horse-looking animals.</p></blockquote><h2 id="the-mscoco-image-captioning-challenge"><a class="header-anchor" href="#the-mscoco-image-captioning-challenge">¶</a>The MSCOCO Image Captioning Challenge</h2><h3 id="metrics"><a class="header-anchor" href="#metrics">¶</a>Metrics</h3><p><strong>CIDER</strong>:All the automatic metrics to correlate with each other quite strongly</p><h3 id="improvements-over-cvpr15-model"><a class="header-anchor" href="#improvements-over-cvpr15-model">¶</a>Improvements Over CVPR15 Model</h3><h4 id="image-model-improvement"><a class="header-anchor" href="#image-model-improvement">¶</a>Image Model Improvement</h4><p>GoogleLeNet -&gt; <em>Batch Normalization</em></p><h4 id="image-model-fine-tuning"><a class="header-anchor" href="#image-model-fine-tuning">¶</a>Image Model Fine Tuning</h4><p>For training:</p><ul><li>To avoid overfitting, initialized the image convolutional network with a pretrained model.</li><li>Fix its parameters and only train the LSTM part of the model on the MSCOCO training set.</li></ul><p>For fine tuning:</p><ul><li>Fine tuning the image model must be carried after the LSTM parameters have settled on a good language model. <strong>Why?</strong> Jointly training both, the noise in the initial gradients coming from the LSTM into the image model corrupted the CNN and would never recover.</li><li>Train for about 500K steps (freezing the CNN parameters), and then switch to jointly train the model for an additional 100K steps.</li><li>Improvements achieved by this was 1 BLEU-4 point. <strong>Why?</strong></li></ul><blockquote><p>More importantly, this change allowed the model to transfer information from the image to the language which was likely not possible due to the insufficient coverage of the ImageNet label space. For instance, after the change we found many examples where we predict the right colors, e.g. “A blue and yellow train …”. It is plausible that the toplayer CNN activations are overtrained on ImageNet-specific classes and could throw away interesting features (such as color), thus the caption generation model may not output words corresponding to those features, without fine tuning the image model.</p></blockquote><h4 id="scheduled-sampling"><a class="header-anchor" href="#scheduled-sampling">¶</a>Scheduled Sampling</h4><p><strong>A curriculum learning strategy</strong></p><ul><li>Gently change the training process from a fully guided scheme using the true previous word, towards a less guided scheme which mostly uses the model generated word instead.</li><li>It improved up to 1.5 BLEU-4 points</li></ul><h4 id="ensembling"><a class="header-anchor" href="#ensembling">¶</a>Ensembling</h4><ul><li>an ensemble of 5 models trained with Scheduled Sampling and 10 models trained with finetuning the image model.</li><li>further improved the results by 1.5 BLEU-4 points. <strong>Why?</strong> To be discovered.</li></ul><h4 id="beam-size-reduction"><a class="header-anchor" href="#beam-size-reduction">¶</a>Beam Size Reduction</h4><ul><li>The best beam size turned out to be small:<strong>3</strong>. <strong>Why?</strong></li></ul><blockquote><p>Hence, if the model was well trained and the likelihood was aligned with human judgement, increasing the beam size should always yield better sentences. The fact that we obtained the best performance with a relatively small beam size is an indication that <strong>either the model has overfitted</strong> or <strong>the objective function used to train it (likelihood) is not aligned with human judgement</strong>.</p></blockquote><ul><li>By <strong>reducing</strong> the beam sizem, the novelty of generated sentences <strong>increased</strong>. <strong>Why?</strong></li></ul><blockquote><p>This hypothesis supports the fact that the model has overfitted to the training set.</p></blockquote><ul><li><p>Reduced beam size technique as another way to regularize (by adding some noise to the inference process).</p></li><li><p>The single change improved CIDER score the most, yielding more than 2 BLEU-4 points improvement.</p></li></ul><h2 id="conclusion"><a class="header-anchor" href="#conclusion">¶</a>Conclusion</h2>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Caption </tag>
            
            <tag> Paper Notes </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>My CTF Journey:i春秋《戏说春秋》</title>
      <link href="/2016/12/05/My-CTF-Journey-i%E6%98%A5%E7%A7%8B%E3%80%8A%E6%88%8F%E8%AF%B4%E6%98%A5%E7%A7%8B%E3%80%8B/"/>
      <url>/2016/12/05/My-CTF-Journey-i%E6%98%A5%E7%A7%8B%E3%80%8A%E6%88%8F%E8%AF%B4%E6%98%A5%E7%A7%8B%E3%80%8B/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="第一关-图穷匕见"><a class="header-anchor" href="#第一关-图穷匕见">¶</a>第一关 图穷匕见</h2><a id="more"></a><p>题目给了一张图<br><img src="http://static2.ichunqiu.com/icq/resources//fileupload/hackgame/XSCQ/wenzi/3.jpg" alt="Alt text"><br>string 打开最后有一个字符串：</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">%e6</span><span class="symbol">%b2</span><span class="symbol">%a1</span><span class="symbol">%e9</span><span class="symbol">%94</span><span class="symbol">%99</span><span class="symbol">%e4</span><span class="symbol">%bd</span><span class="symbol">%a0</span><span class="symbol">%e7</span><span class="symbol">%9</span>a<span class="symbol">%84</span><span class="symbol">%e6</span><span class="symbol">%89</span><span class="symbol">%93</span><span class="symbol">%e5</span><span class="symbol">%bc</span><span class="symbol">%80</span><span class="symbol">%e6</span><span class="symbol">%96</span><span class="symbol">%b9</span><span class="symbol">%e5</span><span class="symbol">%bc</span><span class="symbol">%8</span>f<span class="symbol">%e7</span><span class="symbol">%9</span>a<span class="symbol">%84</span><span class="symbol">%e6</span><span class="symbol">%ad</span><span class="symbol">%a3</span><span class="symbol">%e7</span><span class="symbol">%a1</span><span class="symbol">%ae</span><span class="symbol">%e7</span><span class="symbol">%9</span>a<span class="symbol">%84</span><span class="symbol">%ef</span><span class="symbol">%bc</span><span class="symbol">%8</span><span class="keyword">c</span><span class="symbol">%e9</span><span class="symbol">%80</span><span class="symbol">%9</span>a<span class="symbol">%e5</span><span class="symbol">%be</span><span class="symbol">%80</span><span class="symbol">%e4</span><span class="symbol">%b8</span><span class="symbol">%8</span>b<span class="symbol">%e4</span><span class="symbol">%b8</span><span class="symbol">%80</span><span class="symbol">%e5</span><span class="symbol">%85</span><span class="symbol">%b3</span><span class="symbol">%e7</span><span class="symbol">%9</span>a<span class="symbol">%84</span>key<span class="symbol">%e6</span><span class="symbol">%98</span><span class="symbol">%af</span><span class="symbol">%ef</span><span class="symbol">%bc</span><span class="symbol">%9</span>a<span class="symbol">%e8</span><span class="symbol">%8</span>a<span class="symbol">%9</span>d<span class="symbol">%e9</span><span class="symbol">%ba</span><span class="symbol">%bb</span><span class="symbol">%e5</span><span class="symbol">%bc</span><span class="symbol">%80</span><span class="symbol">%e9</span><span class="symbol">%97</span><span class="symbol">%a8</span></span><br></pre></td></tr></table></figure><p>使用url编码解码器得到：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">没错你的打开方式的正确的，通往下一关的<span class="built_in">key</span>是：芝麻开门</span><br></pre></td></tr></table></figure><h2 id="第二关-纸上谈兵"><a class="header-anchor" href="#第二关-纸上谈兵">¶</a>第二关 纸上谈兵</h2><p>一开始什么也没有。查看网页源代码，找到</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"hidden"</span> <span class="attr">value</span>=<span class="string">"W1ZbTDEYCh0cDRkcGw"</span> <span class="attr">name</span>=<span class="string">"coursescoreid"</span> <span class="attr">id</span>=<span class="string">"coursescoreid"</span>&gt;</span></span><br></pre></td></tr></table></figure><p>把value值base64解码后提交，不对。<br>无奈上网搜了一下，才发现原来在页面里有一行非常长，被挡住了：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通关秘钥是一个贝丝第<span class="number">64</span>代的人设计的，你能解开它吗？<span class="number">5</span>be05ouJ5be05ouJ5bCP6a2U5LuZ</span><br></pre></td></tr></table></figure><p>解码后得到答案：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">巴拉巴拉小魔仙</span><br></pre></td></tr></table></figure><p>服了…以后看代码还是要仔细。</p><h2 id="第三关-窃符救赵"><a class="header-anchor" href="#第三关-窃符救赵">¶</a>第三关 窃符救赵</h2><p>给了一张图：<br><img src="http://static2.ichunqiu.com/icq/resources//fileupload/hackgame/XSCQ/wenzi/7.jpg" alt="Alt text"><br>binwalk之，发现zip的痕迹。重命名为7.zip,解压后得到dh.jpg.<br>然后就不知道该怎么办了，strings无迹象，stegdetect也没有结果。Key在哪里？<br>无奈百度之，给这脑洞跪了：原来是要把图片放到百度识图里，将搜索得到的“杜虎符”三个字作为Key提交…</p><h2 id="第四关-老马识途"><a class="header-anchor" href="#第四关-老马识途">¶</a>第四关 老马识途</h2><p><img src="http://static2.ichunqiu.com/icq/resources//fileupload/hackgame/XSCQ/wenzi/ls.jpg" alt="Alt text"><br>猪圈密码。<br><img src="http://e.hiphotos.baidu.com/baike/w%3D268/sign=46c6982108d79123e0e0937295345917/91ef76c6a7efce1b115000e3ab51f3deb48f65a4.jpg" alt="Alt text"><br>得到HORSE.坑爹的是还要翻译成“马”。</p><h2 id="第五关-东施效颦"><a class="header-anchor" href="#第五关-东施效颦">¶</a>第五关 东施效颦</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">趁着西施跟随父母回乡下走亲戚时，东施冒充西施与范蠡对唱情歌，</span><br><span class="line"></span><br><span class="line">“啊哈啊啊，哈哈哈哈哈，啊啊啊哈，啊”</span><br><span class="line"></span><br><span class="line">但是范蠡根本就不上当，毫不留情的指责东施说，你只知道冒充唱歌，但是你跟不懂西施在歌里对我说的话。那么问题来了，西施的歌里到底藏着什么秘密呢。</span><br></pre></td></tr></table></figure><p>很显然这是一段Morse Code，“哈”为长，“啊”为短，四个字母为LOVE。需要注意的是“哈哈哈哈哈”似乎有误，应当是“哈哈哈”。</p><h2 id="第六关-大义灭亲"><a class="header-anchor" href="#第六关-大义灭亲">¶</a>第六关 大义灭亲</h2><p>讲的是石碏(que)大义灭亲的故事。<br><img src="http://pic3.zhimg.com/v2-05df26b1b043e1c85671625dd92bfca6_r.jpg" alt="Alt text"><br>又到了喜闻乐见的猜密码环节。<br>我猜不出来。我百度了。密码是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shique719</span><br></pre></td></tr></table></figure><p>719是卫桓公被弑的年份…U fking kidding me?</p><h2 id="第七关-三令五申"><a class="header-anchor" href="#第七关-三令五申">¶</a>第七关 三令五申</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“这个盒子里面的是盛放军令状，平时将军写好军令就直接放在里面，我和其他副将都能打开这个盒子查看，然后执行里面的命令。而其他人无法解开这个盒子查看里面的军令，如果大王能够解开这个盒子，在里面放入一张饶恕两位队长的军令，您的爱姬自然能够获救。”</span><br></pre></td></tr></table></figure><p>要获得开启盒子的密钥。也是没想出来，居然是权限值！<br>将军是文件的所有者，具有读取、写入和执行权限，副将没有写入权限，只有读取和执行，其他人无权限，因此计算出的权限值为750.这个套路实在是深。</p>]]></content>
      
      <categories>
          
          <category> Hacking </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CTF </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>My CTF Journey: SUCTF招新：Web</title>
      <link href="/2016/11/14/My-CTF-Journey-SUCTF%E6%8B%9B%E6%96%B0-Web/"/>
      <url>/2016/11/14/My-CTF-Journey-SUCTF%E6%8B%9B%E6%96%B0-Web/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>本文参照了https://blog.rexskz.info/?p=422</p><a id="more"></a><h2 id="1-flag在哪？"><a class="header-anchor" href="#1-flag在哪？">¶</a>1.flag在哪？</h2><p>打开网址，BurpSuire抓包发现Cookie:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cookie:flag=suctf%7BThi5_i5_a_baby_w3b%7D</span><br></pre></td></tr></table></figure><h2 id="2-编码"><a class="header-anchor" href="#2-编码">¶</a>2.编码</h2><p>打开网址，有输入框，submit按钮被disable掉。抓包发现Password:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Password: <span class="attribute">VmxST1ZtVlZNVFpVVkRBOQ</span>==</span><br></pre></td></tr></table></figure><p>用Base64多次解密后得到Su233.由于submit被disable，直接把Su233作为参数password在url里提交即可：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url?<span class="attribute">password</span>=Su233</span><br></pre></td></tr></table></figure><p>另一个方法是使用Chrome修改网页使按钮变得可用，方法待查。</p><h2 id="3-xss1"><a class="header-anchor" href="#3-xss1">¶</a>3.XSS1</h2><p>提交后过滤了script串，所以不能用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">闭合源代码中的```&lt;pre&gt;```标签，利用onerror属性：</span><br><span class="line">```plain</span><br><span class="line">&lt;/pre&gt;&lt;img src=# onerror=alert(1)&gt;</span><br></pre></td></tr></table></figure></p><h2 id="4-php是世界上最好的语言"><a class="header-anchor" href="#4-php是世界上最好的语言">¶</a>4.PHP是世界上最好的语言</h2><p>查看源代码：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="keyword">isset</span>($_GET[<span class="string">"password"</span>]) &amp;&amp; md5($_GET[<span class="string">"password"</span>]) == <span class="string">"0"</span>)</span><br><span class="line">    <span class="keyword">echo</span> file_get_contents(<span class="string">"/opt/flag.txt"</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">echo</span> file_get_contents(<span class="string">"xedni.php"</span>);</span><br></pre></td></tr></table></figure><p>此处利用了PHP语言md5()函数的缺陷：所有hash后为0e开头的串均会被解释为0，因而任意两个hash后为0e开头的串相等，这就是php的Magic Hash漏洞。<br>参考链接：<a href="https://www.whitehatsec.com/blog/magic-hashes/" target="_blank" rel="noopener">https://www.whitehatsec.com/blog/magic-hashes/</a></p><h2 id="5-つロ-乾杯"><a class="header-anchor" href="#5-つロ-乾杯">¶</a>5.( ゜- ゜)つロ 乾杯~</h2><p>给了一大堆形似颜文字的字符，其实是AAencode编码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ﾟωﾟﾉ= /｀ｍ´）ﾉ ~┻━┻   //*´∇｀*/ [&apos;_&apos;]; o=(ﾟｰﾟ)  =_=3; c=(ﾟΘﾟ) =(ﾟｰﾟ)-(ﾟｰﾟ); (ﾟДﾟ) =(ﾟΘﾟ)= (o^_^o)/ (o^_^o);(ﾟДﾟ)=&#123;ﾟΘﾟ: &apos;_&apos; ,ﾟωﾟﾉ : ((ﾟωﾟﾉ==3) +&apos;_&apos;) [ﾟΘﾟ] ,ﾟｰﾟﾉ :(ﾟωﾟﾉ+ &apos;_&apos;)[o^_^o -(ﾟΘﾟ)] ,ﾟДﾟﾉ:((ﾟｰﾟ==3) +&apos;_&apos;)[ﾟｰﾟ] &#125;; (ﾟДﾟ) [ﾟΘﾟ] =((ﾟωﾟﾉ==3) +&apos;_&apos;) [c^_^o];(ﾟДﾟ) [&apos;c&apos;] = ((ﾟДﾟ)+&apos;_&apos;) [ (ﾟｰﾟ)+(ﾟｰﾟ)-(ﾟΘﾟ) ];(ﾟДﾟ) [&apos;o&apos;] = ((ﾟДﾟ)+&apos;_&apos;) [ﾟΘﾟ];(ﾟoﾟ)=(ﾟДﾟ) [&apos;c&apos;]+(ﾟДﾟ) [&apos;o&apos;]+(ﾟωﾟﾉ +&apos;_&apos;)[ﾟΘﾟ]+ ((ﾟωﾟﾉ==3) +&apos;_&apos;) [ﾟｰﾟ] + ((ﾟДﾟ) +&apos;_&apos;) [(ﾟｰﾟ)+(ﾟｰﾟ)]+ ((ﾟｰﾟ==3) +&apos;_&apos;) [ﾟΘﾟ]+((ﾟｰﾟ==3) +&apos;_&apos;) [(ﾟｰﾟ) - (ﾟΘﾟ)]+(ﾟДﾟ) [&apos;c&apos;]+((ﾟДﾟ)+&apos;_&apos;) [(ﾟｰﾟ)+(ﾟｰﾟ)]+ (ﾟДﾟ) [&apos;o&apos;]+((ﾟｰﾟ==3) +&apos;_&apos;) [ﾟΘﾟ];(ﾟДﾟ) [&apos;_&apos;] =(o^_^o) [ﾟoﾟ] [ﾟoﾟ];(ﾟεﾟ)=((ﾟｰﾟ==3) +&apos;_&apos;) [ﾟΘﾟ]+ (ﾟДﾟ) .ﾟДﾟﾉ+((ﾟДﾟ)+&apos;_&apos;) [(ﾟｰﾟ) + (ﾟｰﾟ)]+((ﾟｰﾟ==3) +&apos;_&apos;) [o^_^o -ﾟΘﾟ]+((ﾟｰﾟ==3) +&apos;_&apos;) [ﾟΘﾟ]+ (ﾟωﾟﾉ +&apos;_&apos;) [ﾟΘﾟ]; (ﾟｰﾟ)+=(ﾟΘﾟ); (ﾟДﾟ)[ﾟεﾟ]=&apos;\\&apos;; (ﾟДﾟ).ﾟΘﾟﾉ=(ﾟДﾟ+ ﾟｰﾟ)[o^_^o -(ﾟΘﾟ)];(oﾟｰﾟo)=(ﾟωﾟﾉ +&apos;_&apos;)[c^_^o];(ﾟДﾟ) [ﾟoﾟ]=&apos;\&quot;&apos;;(ﾟДﾟ) [&apos;_&apos;] ( (ﾟДﾟ) [&apos;_&apos;] (ﾟεﾟ+(ﾟДﾟ)[ﾟoﾟ]+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ (o^_^o)+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ (ﾟｰﾟ)+ (ﾟДﾟ)[ﾟεﾟ]+(ﾟΘﾟ)+ (o^_^o)+ ((ﾟｰﾟ) + (ﾟΘﾟ))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (o^_^o))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟεﾟ]+((ﾟｰﾟ) + (ﾟΘﾟ))+ ((o^_^o) +(o^_^o))+ (ﾟДﾟ)[ﾟoﾟ]) (ﾟΘﾟ)) (&apos;_&apos;);</span><br></pre></td></tr></table></figure><p>在线decoder:<a href="https://cat-in-136.github.io/2010/12/aadecode-decode-encoded-as-aaencode.html" target="_blank" rel="noopener">https://cat-in-136.github.io/2010/12/aadecode-decode-encoded-as-aaencode.html</a><br>解码得到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+[---------&gt;++&lt;]&gt;+.++.+[-&gt;+++&lt;]&gt;+.-[---&gt;+&lt;]&gt;--.+++[-&gt;+++&lt;]&gt;+.+[-----&gt;+&lt;]&gt;.-[-&gt;+++++&lt;]&gt;-.-[---&gt;++&lt;]&gt;+.+[--&gt;+++&lt;]&gt;++.+++++++++.-----------.-[--&gt;+&lt;]&gt;-.++[-&gt;++&lt;]&gt;.+.------.++.+++++++++++++.----------.-----.+++.[---&gt;+&lt;]&gt;----.+++[-&gt;+++&lt;]&gt;++.+[--&gt;+&lt;]&gt;.-[-----&gt;+&lt;]&gt;--.--------.---[-&gt;+++&lt;]&gt;+..[--&gt;+++++&lt;]&gt;++.&gt;--[--&gt;+++&lt;]&gt;.</span><br></pre></td></tr></table></figure><p>这是一段Brainfuck。再解码：<a href="https://copy.sh/brainfuck/" target="_blank" rel="noopener">https://copy.sh/brainfuck/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">suctf&#123;aAenc0de_and_bra1nf**k&#125;</span><br></pre></td></tr></table></figure><h2 id="6-你是谁？你从哪里来？"><a class="header-anchor" href="#6-你是谁？你从哪里来？">¶</a>6.你是谁？你从哪里来？</h2><p>该页面只允许从http://www.suctf.com访问，利用http的Origin和X-Forwarded-For字段：</p><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Origin</span>: http://www.suctf.com</span><br><span class="line"><span class="attribute">X-Forwarded-For</span>: xxx.xxx.xxx.xxx</span><br></pre></td></tr></table></figure><h2 id="7-xss2"><a class="header-anchor" href="#7-xss2">¶</a>7.XSS2</h2><p>名称是XSS2，然而用的却是隐写…题目给出网址：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:<span class="regexp">//</span>xxx.xxx.xxx.xxx<span class="regexp">/44b22f2bf7c7cfa05c351a5bf228fee0/</span>xss2.php</span><br></pre></td></tr></table></figure><p>删除xss2.php后可以看到目录下有.tif图片，打开二进制即可找到flag。</p>]]></content>
      
      <categories>
          
          <category> Hacking </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CTF </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>My CTF Journey: WeChall  Writeup(1)</title>
      <link href="/2016/07/16/My-CTF-Journey-WeChall-Writeup-1/"/>
      <url>/2016/07/16/My-CTF-Journey-WeChall-Writeup-1/</url>
      <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>以前想写一些技术类的内容总是无从下手，因为多少会有些枯燥。刚好最近的闲暇时光在做CTF题，都是一些比较有意思的挑战（类似于谜题），决定从这里试水。有兴趣的朋友可以到www.wechall.net试一试。</p><a id="more"></a><h3 id="prime-factory"><a class="header-anchor" href="#prime-factory">¶</a>Prime Factory</h3><blockquote><p>Your task is simple:<br>Find the first two primes above 1 million, whose separate digit sums are also prime.<br>As example take 23, which is a prime whose digit sum, 5, is also prime.<br>The solution is the concatination of the two numbers,<br>Example: If the first number is 1,234,567<br>and the second is 8,765,432,<br>your solution is 12345678765432</p></blockquote><p>这道题比较简单。找到大于一百万且各位数字相加后也为质数的前两个质数，把这两个数串接起来就是答案。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">is_prime</span><span class="params">(<span class="keyword">int</span> i)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">digitsum</span><span class="params">(<span class="keyword">int</span> i)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,x1,x2,count=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">1000001</span>;count&lt;<span class="number">2</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(is_prime(i))</span><br><span class="line">            <span class="keyword">if</span>(is_prime(digitsum(i)))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,i);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">is_prime</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t;</span><br><span class="line">    <span class="keyword">for</span>(t=<span class="number">2</span>;t&lt;<span class="built_in">ceil</span>(<span class="built_in">sqrt</span>(i))+<span class="number">1</span>;t++)</span><br><span class="line">        <span class="keyword">if</span>(i%t==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">digitsum</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum=<span class="number">0</span>,t=<span class="number">10</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&gt;=<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        sum+=i%t;</span><br><span class="line">        i=<span class="built_in">floor</span>(i/<span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>C语言写起来比较复杂，要是用Python的话就会简洁很多：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line">t = <span class="number">1000000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isPrime</span><span class="params">(x)</span>:</span></span><br><span class="line">    t = sp.check_output((<span class="string">"factor %d"</span> % x).split())</span><br><span class="line">    <span class="keyword">if</span> t.split()[<span class="number">1</span>] == str(x):</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    t += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> isPrime(t) <span class="keyword">and</span> isPrime(sum([int(x) <span class="keyword">for</span> x <span class="keyword">in</span> str(t)])):</span><br><span class="line">        <span class="keyword">print</span> t</span><br></pre></td></tr></table></figure><p>（这段代码来自<a href="http://winkar.github.io/2015/01/24/wechall.html" target="_blank" rel="noopener">陈文青的主页</a>）<br>是不是看起来简洁多了？掌握一门脚本语言真的很重要啊。<br>p.s. 其实还有更简便的：上网搜索质数表，翻到100万左右的数，挨个校验一下很快就能找到答案。某种程度上我认为这是更hacker的方法。</p><h3 id="training-get-sourced"><a class="header-anchor" href="#training-get-sourced">¶</a>Training: Get Sourced</h3><blockquote><p>The solution is hidden in this page<br><em>Use View Sourcecode to get it</em></p></blockquote><p>很基础的CTF题。人家都讲了去看源代码，那就在网页上右键-&gt;查看源代码，拖动到最后一行得到答案。</p><blockquote><p>You are looking for this password: html_sourcecode</p></blockquote><h3 id="training-stegano-i"><a class="header-anchor" href="#training-stegano-i">¶</a>Training: Stegano I</h3><blockquote><p>This is the most basic image stegano I can think of.</p></blockquote><p>然后给了一张视线模糊的图。<br>Stegano是Steganography的短写，意思是隐写术（<a href="https://en.wikipedia.org/wiki/Steganography" target="_blank" rel="noopener">Wiki:Stegenography</a>）。此处是通过一定方法在图片中隐藏了某些信息，也是常见的题目。第一反应是先查看十六进制数据。不过要是懒的话可以先右键用记事本打开，就能找到答案:</p><blockquote><p>Look what the hex-edit revealed: passwd:steganoI</p></blockquote><h3 id="training-crypto-caesar-i"><a class="header-anchor" href="#training-crypto-caesar-i">¶</a>Training: Crypto - Caesar I</h3><blockquote><p>As on most challenge sites, there are some beginner cryptos, and often you get started with the good old caesar cipher.<br>I welcome you to the WeChall style of these training challenges :)<br>Enjoy!<br>接着是下面一段密文：<br>BPM YCQKS JZWEV NWF RCUXA WDMZ BPM TIHG LWO WN KIMAIZ IVL GWCZ CVQYCM AWTCBQWV QA JZKMMUKNQAJP</p></blockquote><p>凯撒密码（<a href="https://en.wikipedia.org/wiki/Caesar_cipher" target="_blank" rel="noopener">Wiki: Caesar cipher</a>）是密码学里的入门科目,基本原理就是把字母表顺次移动一定的位数k后重新映射。比如k=3时，密文D就代表A，you加密后就变成了brx.<br>实在懒得写程序了，网上搜了一个现成的解码器，只不过k的值要手工试。得到答案:</p><blockquote><p><em>我忘了，也懒得再做一遍抱歉…</em></p></blockquote><p>当然也有简洁但高深一点的做法，利用Linux系统下的tr命令：</p><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="string">"VJG SWKEM DTQYP HQZ LWORU QXGT VJG NCBA FQI QH ECGUCT CPF AQWT WPKSWG UQNWVKQP KU UDCJRKRJUROQ"</span> | tr [A-<span class="keyword">Z</span>] [<span class="keyword">Y</span>-ZA-<span class="keyword">X</span>]</span><br></pre></td></tr></table></figure><p>诺，就是这样。</p><p>今天就到这里吧。写下这些的初衷是记录自己的爱好同时也是日常娱乐项目。专业外人士能看到这里真的很不容易，衷心地祝您晚安。</p><p><em>不爱看也没事儿，反正我还(ke)是(neng)会继续写的。</em></p>]]></content>
      
      <categories>
          
          <category> Hacking </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CTF </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
